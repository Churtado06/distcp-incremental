############################################################
Create Dataproc Clusters with cluster pool
############################################################

gcloud config set project poc-incremental-334512
export PROJECT_ID=poc-incremental-334512
export COMPOSER_ENVIRONMENT=distcp-orch

### Create Composer environment

gcloud composer environments create $COMPOSER_ENVIRONMENT \
    --location us-central1 \
    --zone=us-central1-a \
    --airflow-configs=api-auth_backend=airflow.api.auth.backend.default


### Get airflowUri - AIRFLOW_URL

AIRFLOW_URL=$(gcloud composer environments describe $COMPOSER_ENVIRONMENT --location us-central1 | grep airflowUri | cut -d":" -f2-)

### Get the client_id of the IAM proxy. Replace AIRFLOW_URL with the URL of the Airflow web interface.

curl -v $AIRFLOW_URL 2>&1 >/dev/null | grep -o "client_id\=[A-Za-z0-9-]*\.apps\.googleusercontent\.com"

*** Replace this client_id in the cloud function code (main.py)

### Get webserver-id

echo "${AIRFLOW_URL/"https://"/}"  | sed 's/.appspot.com//g' 

*** Replace this webserver_id in the cloud function code (main.py)


#######################################################
Copy DAG to Airflow
#######################################################

gcloud composer environments storage dags import --environment distcp-orch --location us-central1 --source airflow_parallel_distcp_dag.py

### Create the pull clusters

gcloud dataproc clusters create cluster-pull-1 --region us-central1 --zone us-central1-a --single-node --master-machine-type n1-standard-2 --master-boot-disk-size 200 --image-version 2.0-debian10 --labels cluster-pool=pool-1 --project $PROJECT_ID
gcloud dataproc clusters create cluster-pull-2 --region us-central1 --zone us-central1-a --single-node --master-machine-type n1-standard-2 --master-boot-disk-size 200 --image-version 2.0-debian10 --labels cluster-pool=pool-1 --project $PROJECT_ID


############################################################
Initialization actions / dataset / create cluster on-premise
############################################################

## Define bucket names

export AUDIT_BUCKET=audit-file-distcp-bucket
export DISTCP_BUCKET=dest-distcp-bucket-demo
export SOURCE_FILES_DATA_BUCKET=source-files-data-onprem

## Create buckets
gsutil mb -b on gs://$AUDIT_BUCKET
gsutil mb -b on gs://$DISTCP_BUCKET
gsutil mb -b on gs://$SOURCE_FILES_DATA_BUCKET

## Copy dummy file
gsutil cp ./input_data/titanic.csv gs://$SOURCE_FILES_DATA_BUCKET/input_data/titanic.csv

####################################################
Deploying cloud function
####################################################

### Define bucket names
export CLOUD_FUNCTION_BUCKET=cloud-function-bucket
export TRIGGER_BUCKET_NAME=audit-file-distcp-bucket

## Create buckets
gsutil mb -b on gs://$CLOUD_FUNCTION_BUCKET

### Create zip
** move to cloud_function_trigger_dag folder

cd cloud_function_trigger_dag/

zip -r main.zip main.py requirements.txt

### Copy zip file to bucket

gsutil cp ./main.zip gs://$CLOUD_FUNCTION_BUCKET/main.zip

###Deploy cloud function
gcloud functions deploy trigger_dag_function --entry-point trigger_dag --runtime python37 --trigger-resource $TRIGGER_BUCKET_NAME --trigger-event google.storage.object.finalize --source gs://$CLOUD_FUNCTION_BUCKET/main.zip


----------------- modify in audit_hdfs_script.sh => USER NAME GOOGLE ----------------
cd ..
gsutil cp audit_hdfs_script.sh gs://$SOURCE_FILES_DATA_BUCKET/audit_hdfs_script.sh
gsutil cp initialize_onprem_cluster.sh gs://$SOURCE_FILES_DATA_BUCKET/initialize_onprem_cluster.sh


## Create the on-premise simulation cluster

gcloud dataproc clusters create cluster-src --region us-central1 --zone us-central1-a --single-node --master-machine-type n1-standard-2 --master-boot-disk-size 200 --image-version 2.0-debian10 --project $PROJECT_ID --initialization-actions=SOURCE_FILES_DATA_BUCKET/initialize_onprem_cluster.sh


# Remove cluster src

gcloud dataproc clusters delete cluster-src --region us-central1